{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Solving SVM Dual - Question 2**"
      ],
      "metadata": {
        "id": "VY97ijn2Q7qT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Library**"
      ],
      "metadata": {
        "id": "AvkhqyW35-rW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from cvxopt import matrix, solvers\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "ToxOhyBs2Gw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**"
      ],
      "metadata": {
        "id": "5IJqezq-6BhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Replace 'your_file_id_here' with the actual file ID\n",
        "file_id = '18I76FlBk2DIli3NbvnvPrG4nqZUETCO1'\n",
        "# Read the CSV file into a DataFrame'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "\n",
        "df = pd.read_csv(download_url)"
      ],
      "metadata": {
        "id": "GKZMqO7a2IgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, :-1].values  # last column is the label\n",
        "y = df.iloc[:, -1].values   # labels"
      ],
      "metadata": {
        "id": "nTlcsriu2Kjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels: 0 to -1 and 1 to 1\n",
        "y = np.where(y == 0, -1, 1)\n",
        "X_train , X_test , y_train , y_test = train_test_split(X , y, test_size = 0.3)"
      ],
      "metadata": {
        "id": "X4CQyqTH2M79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4PQo5KlGmdf",
        "outputId": "d240646a-9431-4d7d-a035-1cebc541e091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.1197387 ,  0.009475  ,  0.2638998 , ...,  0.23404728,\n",
              "         0.00697374,  1.4517994 ],\n",
              "       [ 2.453599  , -0.14742422,  0.28554168, ...,  0.5980694 ,\n",
              "        -0.05288332,  3.6111503 ],\n",
              "       [ 2.1931965 , -0.13986062,  0.29031608, ...,  0.56720823,\n",
              "        -0.02771215,  3.1775737 ],\n",
              "       ...,\n",
              "       [-0.8989769 ,  0.0766169 , -0.39390913, ..., -0.6827197 ,\n",
              "        -0.32803258, -0.87315875],\n",
              "       [ 0.27684978,  0.07935488,  0.19934243, ...,  0.01578434,\n",
              "         0.05318773,  0.07524591],\n",
              "       [-0.23955972,  0.03159761, -0.157269  , ..., -0.25828475,\n",
              "         0.17565508, -0.70011437]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Kernel Functions**"
      ],
      "metadata": {
        "id": "5a495FYE6IS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def gaussian_kernel_matrix(X1, X2, gamma):\n",
        "    \"\"\"Compute the Gaussian (RBF) kernel matrix between two sets of vectors.\"\"\"\n",
        "    X1_sq = np.sum(X1 ** 2, axis=1)\n",
        "    X2_sq = np.sum(X2 ** 2, axis=1)\n",
        "    K = np.exp(-gamma * (np.outer(X1_sq, np.ones(X2.shape[0])) + np.outer(np.ones(X1.shape[0]), X2_sq) - 2 * np.dot(X1, X2.T)))\n",
        "    return K\n",
        "\n",
        "def polynomial_kernel_matrix(X1, X2, p):\n",
        "    \"\"\"Compute the Polynomial kernel matrix between two sets of vectors.\"\"\"\n",
        "    return (np.dot(X1, X2.T) + 1) ** p\n"
      ],
      "metadata": {
        "id": "VD2e2QjR2PPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Solving SVM dual using cvxopt**"
      ],
      "metadata": {
        "id": "hkfbzgL96ODR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Dual Problem using CVXOPT\n",
        "def fit_svm(X, y, C, kernel_func, gamma=None, p=None):\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    # Compute the kernel matrix using the selected kernel function\n",
        "    if kernel_func == 'gaussian':\n",
        "        K = gaussian_kernel_matrix(X, X, gamma)\n",
        "    elif kernel_func == 'polynomial':\n",
        "        K = polynomial_kernel_matrix(X, X, p)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported kernel function. Use 'gaussian' or 'polynomial'.\")\n",
        "\n",
        "    # Set up the parameters for the quadratic programming problem\n",
        "    P = matrix(np.outer(y, y) * K)\n",
        "    q = matrix(-np.ones(n_samples))\n",
        "    G = matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))\n",
        "    h = matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)))\n",
        "    A = matrix(y, (1, n_samples), 'd')\n",
        "    b = matrix(0.0)\n",
        "\n",
        "    # Solve QP problem using CVXOPT\n",
        "    solvers.options['show_progress'] = False\n",
        "    solution = solvers.qp(P, q, G, h, A, b)\n",
        "\n",
        "    alphas = np.ravel(solution['x'])\n",
        "    return alphas\n"
      ],
      "metadata": {
        "id": "OQd_n97T2WjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SVM cross validation for fine_tunning**"
      ],
      "metadata": {
        "id": "PO4JRjAj6dPr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3f-bjVe1s-l",
        "outputId": "289ee410-e3ad-420c-93de-39987b042d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Vectors Count: 225\n",
            "Bias Term: -0.06141373799505384\n",
            "Support Vectors Count: 241\n",
            "Bias Term: -0.12207357036784174\n",
            "Support Vectors Count: 180\n",
            "Bias Term: -0.05950946434825987\n",
            "Support Vectors Count: 192\n",
            "Bias Term: -0.06274276601976365\n",
            "Support Vectors Count: 198\n",
            "Bias Term: -0.15535689601269087\n",
            "Average Score for C=0.1, gamma=0.1, p=2: 0.9171428571428573\n",
            "Support Vectors Count: 225\n",
            "Bias Term: -0.06141373799505384\n",
            "Support Vectors Count: 241\n",
            "Bias Term: -0.12207357036784174\n",
            "Support Vectors Count: 180\n",
            "Bias Term: -0.05950946434825987\n",
            "Support Vectors Count: 192\n",
            "Bias Term: -0.06274276601976365\n",
            "Support Vectors Count: 198\n",
            "Bias Term: -0.15535689601269087\n",
            "Average Score for C=0.1, gamma=0.1, p=3: 0.9171428571428573\n",
            "Support Vectors Count: 225\n",
            "Bias Term: -0.06141373799505384\n",
            "Support Vectors Count: 241\n",
            "Bias Term: -0.12207357036784174\n",
            "Support Vectors Count: 180\n",
            "Bias Term: -0.05950946434825987\n",
            "Support Vectors Count: 192\n",
            "Bias Term: -0.06274276601976365\n",
            "Support Vectors Count: 198\n",
            "Bias Term: -0.15535689601269087\n",
            "Average Score for C=0.1, gamma=0.1, p=5: 0.9171428571428573\n",
            "Support Vectors Count: 273\n",
            "Bias Term: -0.15191772495161324\n",
            "Support Vectors Count: 285\n",
            "Bias Term: -0.1848548258073401\n",
            "Support Vectors Count: 260\n",
            "Bias Term: -0.11961625084972245\n",
            "Support Vectors Count: 265\n",
            "Bias Term: -0.10525041048987908\n",
            "Support Vectors Count: 276\n",
            "Bias Term: -0.16926248635461782\n",
            "Average Score for C=0.1, gamma=0.5, p=2: 0.9171428571428573\n",
            "Support Vectors Count: 273\n",
            "Bias Term: -0.15191772495161324\n",
            "Support Vectors Count: 285\n",
            "Bias Term: -0.1848548258073401\n",
            "Support Vectors Count: 260\n",
            "Bias Term: -0.11961625084972245\n",
            "Support Vectors Count: 265\n",
            "Bias Term: -0.10525041048987908\n",
            "Support Vectors Count: 276\n",
            "Bias Term: -0.16926248635461782\n",
            "Average Score for C=0.1, gamma=0.5, p=3: 0.9171428571428573\n",
            "Support Vectors Count: 273\n",
            "Bias Term: -0.15191772495161324\n",
            "Support Vectors Count: 285\n",
            "Bias Term: -0.1848548258073401\n",
            "Support Vectors Count: 260\n",
            "Bias Term: -0.11961625084972245\n",
            "Support Vectors Count: 265\n",
            "Bias Term: -0.10525041048987908\n",
            "Support Vectors Count: 276\n",
            "Bias Term: -0.16926248635461782\n",
            "Average Score for C=0.1, gamma=0.5, p=5: 0.9171428571428573\n",
            "Support Vectors Count: 318\n",
            "Bias Term: -0.1200037662729022\n",
            "Support Vectors Count: 305\n",
            "Bias Term: -0.16125599715281058\n",
            "Support Vectors Count: 333\n",
            "Bias Term: -0.11998089691496405\n",
            "Support Vectors Count: 337\n",
            "Bias Term: -0.12820992074714624\n",
            "Support Vectors Count: 346\n",
            "Bias Term: -0.18932219232652842\n",
            "Average Score for C=0.1, gamma=1, p=2: 0.9171428571428573\n",
            "Support Vectors Count: 318\n",
            "Bias Term: -0.1200037662729022\n",
            "Support Vectors Count: 305\n",
            "Bias Term: -0.16125599715281058\n",
            "Support Vectors Count: 333\n",
            "Bias Term: -0.11998089691496405\n",
            "Support Vectors Count: 337\n",
            "Bias Term: -0.12820992074714624\n",
            "Support Vectors Count: 346\n",
            "Bias Term: -0.18932219232652842\n",
            "Average Score for C=0.1, gamma=1, p=3: 0.9171428571428573\n",
            "Support Vectors Count: 318\n",
            "Bias Term: -0.1200037662729022\n",
            "Support Vectors Count: 305\n",
            "Bias Term: -0.16125599715281058\n",
            "Support Vectors Count: 333\n",
            "Bias Term: -0.11998089691496405\n",
            "Support Vectors Count: 337\n",
            "Bias Term: -0.12820992074714624\n",
            "Support Vectors Count: 346\n",
            "Bias Term: -0.18932219232652842\n",
            "Average Score for C=0.1, gamma=1, p=5: 0.9171428571428573\n",
            "Support Vectors Count: 166\n",
            "Bias Term: 0.6435627173542872\n",
            "Support Vectors Count: 409\n",
            "Bias Term: 0.4983150304508731\n",
            "Support Vectors Count: 158\n",
            "Bias Term: 0.514948762042451\n",
            "Support Vectors Count: 145\n",
            "Bias Term: 0.39672585059097915\n",
            "Support Vectors Count: 146\n",
            "Bias Term: 0.494887120936689\n",
            "Average Score for C=1, gamma=0.1, p=2: 0.9114285714285714\n",
            "Support Vectors Count: 166\n",
            "Bias Term: 0.6435627173542872\n",
            "Support Vectors Count: 409\n",
            "Bias Term: 0.4983150304508731\n",
            "Support Vectors Count: 158\n",
            "Bias Term: 0.514948762042451\n",
            "Support Vectors Count: 145\n",
            "Bias Term: 0.39672585059097915\n",
            "Support Vectors Count: 146\n",
            "Bias Term: 0.494887120936689\n",
            "Average Score for C=1, gamma=0.1, p=3: 0.9114285714285714\n",
            "Support Vectors Count: 166\n",
            "Bias Term: 0.6435627173542872\n",
            "Support Vectors Count: 409\n",
            "Bias Term: 0.4983150304508731\n",
            "Support Vectors Count: 158\n",
            "Bias Term: 0.514948762042451\n",
            "Support Vectors Count: 145\n",
            "Bias Term: 0.39672585059097915\n",
            "Support Vectors Count: 146\n",
            "Bias Term: 0.494887120936689\n",
            "Average Score for C=1, gamma=0.1, p=5: 0.9114285714285714\n",
            "Support Vectors Count: 222\n",
            "Bias Term: -0.44958067140004565\n",
            "Support Vectors Count: 174\n",
            "Bias Term: -0.4701678197022675\n",
            "Support Vectors Count: 299\n",
            "Bias Term: -0.13705008202706523\n",
            "Support Vectors Count: 194\n",
            "Bias Term: -0.15462652109093708\n",
            "Support Vectors Count: 188\n",
            "Bias Term: -0.35863199614923713\n",
            "Average Score for C=1, gamma=0.5, p=2: 0.9185714285714287\n",
            "Support Vectors Count: 222\n",
            "Bias Term: -0.44958067140004565\n",
            "Support Vectors Count: 174\n",
            "Bias Term: -0.4701678197022675\n",
            "Support Vectors Count: 299\n",
            "Bias Term: -0.13705008202706523\n",
            "Support Vectors Count: 194\n",
            "Bias Term: -0.15462652109093708\n",
            "Support Vectors Count: 188\n",
            "Bias Term: -0.35863199614923713\n",
            "Average Score for C=1, gamma=0.5, p=3: 0.9185714285714287\n",
            "Support Vectors Count: 222\n",
            "Bias Term: -0.44958067140004565\n",
            "Support Vectors Count: 174\n",
            "Bias Term: -0.4701678197022675\n",
            "Support Vectors Count: 299\n",
            "Bias Term: -0.13705008202706523\n",
            "Support Vectors Count: 194\n",
            "Bias Term: -0.15462652109093708\n",
            "Support Vectors Count: 188\n",
            "Bias Term: -0.35863199614923713\n",
            "Average Score for C=1, gamma=0.5, p=5: 0.9185714285714287\n",
            "Support Vectors Count: 209\n",
            "Bias Term: -0.3170711690209663\n",
            "Support Vectors Count: 257\n",
            "Bias Term: -0.14137891996327434\n",
            "Support Vectors Count: 284\n",
            "Bias Term: -0.14684651704709548\n",
            "Support Vectors Count: 322\n",
            "Bias Term: -0.25859674874127997\n",
            "Support Vectors Count: 200\n",
            "Bias Term: -0.3840276114017678\n",
            "Average Score for C=1, gamma=1, p=2: 0.9171428571428573\n",
            "Support Vectors Count: 209\n",
            "Bias Term: -0.3170711690209663\n",
            "Support Vectors Count: 257\n",
            "Bias Term: -0.14137891996327434\n",
            "Support Vectors Count: 284\n",
            "Bias Term: -0.14684651704709548\n",
            "Support Vectors Count: 322\n",
            "Bias Term: -0.25859674874127997\n",
            "Support Vectors Count: 200\n",
            "Bias Term: -0.3840276114017678\n",
            "Average Score for C=1, gamma=1, p=3: 0.9171428571428573\n",
            "Support Vectors Count: 209\n",
            "Bias Term: -0.3170711690209663\n",
            "Support Vectors Count: 257\n",
            "Bias Term: -0.14137891996327434\n",
            "Support Vectors Count: 284\n",
            "Bias Term: -0.14684651704709548\n",
            "Support Vectors Count: 322\n",
            "Bias Term: -0.25859674874127997\n",
            "Support Vectors Count: 200\n",
            "Bias Term: -0.3840276114017678\n",
            "Average Score for C=1, gamma=1, p=5: 0.9171428571428573\n",
            "Support Vectors Count: 559\n",
            "Bias Term: 3.0276610910045343\n",
            "Support Vectors Count: 545\n",
            "Bias Term: -13.543921852885024\n",
            "Support Vectors Count: 559\n",
            "Bias Term: 11.345340175804266\n",
            "Support Vectors Count: 545\n",
            "Bias Term: 4.240495358421978\n",
            "Support Vectors Count: 546\n",
            "Bias Term: -10.628289195945436\n",
            "Average Score for C=10, gamma=0.1, p=2: 0.5199999999999999\n",
            "Support Vectors Count: 559\n",
            "Bias Term: 3.0276610910045343\n",
            "Support Vectors Count: 545\n",
            "Bias Term: -13.543921852885024\n",
            "Support Vectors Count: 559\n",
            "Bias Term: 11.345340175804266\n",
            "Support Vectors Count: 545\n",
            "Bias Term: 4.240495358421978\n",
            "Support Vectors Count: 546\n",
            "Bias Term: -10.628289195945436\n",
            "Average Score for C=10, gamma=0.1, p=3: 0.5199999999999999\n",
            "Support Vectors Count: 559\n",
            "Bias Term: 3.0276610910045343\n",
            "Support Vectors Count: 545\n",
            "Bias Term: -13.543921852885024\n",
            "Support Vectors Count: 559\n",
            "Bias Term: 11.345340175804266\n",
            "Support Vectors Count: 545\n",
            "Bias Term: 4.240495358421978\n",
            "Support Vectors Count: 546\n",
            "Bias Term: -10.628289195945436\n",
            "Average Score for C=10, gamma=0.1, p=5: 0.5199999999999999\n",
            "Support Vectors Count: 537\n",
            "Bias Term: 3.081306768257298\n",
            "Support Vectors Count: 344\n",
            "Bias Term: -3.547771530413766\n",
            "Support Vectors Count: 474\n",
            "Bias Term: 1.1013235431410917\n",
            "Support Vectors Count: 492\n",
            "Bias Term: 1.2294398101037673\n",
            "Support Vectors Count: 516\n",
            "Bias Term: -1.7682737018571257\n",
            "Average Score for C=10, gamma=0.5, p=2: 0.5199999999999999\n",
            "Support Vectors Count: 537\n",
            "Bias Term: 3.081306768257298\n",
            "Support Vectors Count: 344\n",
            "Bias Term: -3.547771530413766\n",
            "Support Vectors Count: 474\n",
            "Bias Term: 1.1013235431410917\n",
            "Support Vectors Count: 492\n",
            "Bias Term: 1.2294398101037673\n",
            "Support Vectors Count: 516\n",
            "Bias Term: -1.7682737018571257\n",
            "Average Score for C=10, gamma=0.5, p=3: 0.5199999999999999\n",
            "Support Vectors Count: 537\n",
            "Bias Term: 3.081306768257298\n",
            "Support Vectors Count: 344\n",
            "Bias Term: -3.547771530413766\n",
            "Support Vectors Count: 474\n",
            "Bias Term: 1.1013235431410917\n",
            "Support Vectors Count: 492\n",
            "Bias Term: 1.2294398101037673\n",
            "Support Vectors Count: 516\n",
            "Bias Term: -1.7682737018571257\n",
            "Average Score for C=10, gamma=0.5, p=5: 0.5199999999999999\n",
            "Support Vectors Count: 295\n",
            "Bias Term: 2.7861364520350524\n",
            "Support Vectors Count: 403\n",
            "Bias Term: -1.0282034594104774\n",
            "Support Vectors Count: 243\n",
            "Bias Term: 0.38783152096683554\n",
            "Support Vectors Count: 489\n",
            "Bias Term: 0.8437158269227303\n",
            "Support Vectors Count: 500\n",
            "Bias Term: -3.0728677029253055\n",
            "Average Score for C=10, gamma=1, p=2: 0.6914285714285715\n",
            "Support Vectors Count: 295\n",
            "Bias Term: 2.7861364520350524\n",
            "Support Vectors Count: 403\n",
            "Bias Term: -1.0282034594104774\n",
            "Support Vectors Count: 243\n",
            "Bias Term: 0.38783152096683554\n",
            "Support Vectors Count: 489\n",
            "Bias Term: 0.8437158269227303\n",
            "Support Vectors Count: 500\n",
            "Bias Term: -3.0728677029253055\n",
            "Average Score for C=10, gamma=1, p=3: 0.6914285714285715\n",
            "Support Vectors Count: 295\n",
            "Bias Term: 2.7861364520350524\n",
            "Support Vectors Count: 403\n",
            "Bias Term: -1.0282034594104774\n",
            "Support Vectors Count: 243\n",
            "Bias Term: 0.38783152096683554\n",
            "Support Vectors Count: 489\n",
            "Bias Term: 0.8437158269227303\n",
            "Support Vectors Count: 500\n",
            "Bias Term: -3.0728677029253055\n",
            "Average Score for C=10, gamma=1, p=5: 0.6914285714285715\n",
            "Execution Time: 81.89725828170776 seconds\n",
            "Best Hyperparameters (C, gamma, p): (1, 0.5, 2)\n",
            "Best Cross-Validation Accuracy: 0.9185714285714287\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def cross_validate_svm(X, y, C_values, gamma_values, p_values, kernel_func, k_folds=5):\n",
        "\n",
        "    start_time = time.time()\n",
        "    best_score = 0\n",
        "    best_params = None\n",
        "\n",
        "    for C in C_values:\n",
        "        for gamma in gamma_values:\n",
        "            for p in p_values:\n",
        "                kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "                scores = []\n",
        "\n",
        "                for train_index, test_index in kf.split(X):\n",
        "                    X_train, X_test = X[train_index], X[test_index]\n",
        "                    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "                    alphas = fit_svm(X_train, y_train, C, kernel_func, gamma, p)\n",
        "\n",
        "                    # Extract support vectors\n",
        "                    support_vectors = alphas > 1e-5\n",
        "                    sv_alphas = alphas[support_vectors]\n",
        "                    sv_X = X_train[support_vectors]\n",
        "                    sv_y = y_train[support_vectors]\n",
        "\n",
        "                    # Calculate bias term b using the support vectors\n",
        "                    if kernel_func == 'gaussian':\n",
        "                        K_sv = gaussian_kernel_matrix(sv_X, sv_X, gamma)\n",
        "                    else:\n",
        "                        K_sv = polynomial_kernel_matrix(sv_X, sv_X, p)\n",
        "\n",
        "                    b = np.mean(sv_y - np.sum(sv_alphas * sv_y[:, np.newaxis] * K_sv, axis=0))\n",
        "\n",
        "                    # Debugging output\n",
        "                    print(f\"Support Vectors Count: {len(sv_X)}\")\n",
        "                    print(f\"Bias Term: {b}\")\n",
        "\n",
        "                    # Predict on the test set\n",
        "                    if kernel_func == 'gaussian':\n",
        "                        K_test = gaussian_kernel_matrix(X_test, sv_X, gamma)\n",
        "                    elif kernel_func == 'polynomial':\n",
        "                        K_test = polynomial_kernel_matrix(X_test, sv_X, p)\n",
        "                    else:\n",
        "                        raise ValueError(\"Unsupported kernel function. Use 'gaussian' or 'polynomial'.\")\n",
        "\n",
        "                    # Calculate decision values\n",
        "                    decision_values = np.dot(K_test, sv_alphas * sv_y) + b\n",
        "                    predictions = np.sign(decision_values)\n",
        "\n",
        "                    accuracy = accuracy_score(y_test, predictions)\n",
        "                    scores.append(accuracy)\n",
        "\n",
        "                avg_score = np.mean(scores)\n",
        "                print(f\"Average Score for C={C}, gamma={gamma}, p={p}: {avg_score}\")\n",
        "\n",
        "                if avg_score > best_score:\n",
        "                    best_score = avg_score\n",
        "                    best_params = (C, gamma, p)\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution Time: {execution_time} seconds\")\n",
        "    return best_params, best_score\n",
        "\n",
        "# Define hyperparameter ranges\n",
        "C_values = [0.1, 1, 10 ]\n",
        "gamma_values = [0.1, 0.5, 1]\n",
        "p_values = [2, 3, 5]\n",
        "\n",
        "# Choose the kernel type\n",
        "kernel_type = 'gaussian'  # or 'polynomial'\n",
        "\n",
        "# Perform cross-validation\n",
        "best_params, best_score = cross_validate_svm(X_train, y_train, C_values, gamma_values, p_values, kernel_type)\n",
        "print('\\n')\n",
        "print('-----------------------------------------------------------------------------')\n",
        "print(\"Best Hyperparameters (C, gamma, p):\", best_params)\n",
        "print(\"Best Cross-Validation Accuracy:\", best_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predicition and accuracy**"
      ],
      "metadata": {
        "id": "C0yTCZol6onY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best hyperparameters to train the final SVM model\n",
        "C_best, gamma_best, p_best = best_params\n",
        "alphas_best = fit_svm(X_train, y_train, C_best, kernel_type, gamma_best, p_best)\n",
        "\n",
        "# Extract support vectors (used only for bias calculation and prediction)\n",
        "support_vectors = alphas_best > 1e-5\n",
        "sv_alphas = alphas_best[support_vectors]\n",
        "sv_X = X_train[support_vectors]\n",
        "sv_y = y_train[support_vectors]\n",
        "\n",
        "# Calculate the final bias term b using only the support vectors\n",
        "if kernel_type == 'gaussian':\n",
        "    K_sv = gaussian_kernel_matrix(sv_X, sv_X, gamma_best)\n",
        "else:\n",
        "    K_sv = polynomial_kernel_matrix(sv_X, sv_X, p_best)\n",
        "\n",
        "b_best = np.mean(sv_y - np.sum(sv_alphas * sv_y[:, np.newaxis] * K_sv, axis=0))\n",
        "\n",
        "# Print the optimal bias term and hyperparameters\n",
        "print(\"Optimal bias term:\", b_best)\n",
        "print(f'Best params: C = {C_best}, gamma = {gamma_best}, p = {p_best}')\n",
        "\n",
        "# Calculate the value of the objective function using the full set of training alphas\n",
        "if kernel_type == 'gaussian':\n",
        "    K_all = gaussian_kernel_matrix(X_train, X_train, gamma_best)\n",
        "else:\n",
        "    K_all = polynomial_kernel_matrix(X_train, X_train, p_best)\n",
        "\n",
        "# Objective function calculation: sum of all alphas minus 1/2 * sum of all pairs of (alpha_i * alpha_j * y_i * y_j * K(x_i, x_j))\n",
        "objective_value = np.sum(alphas_best) - 0.5 * np.sum(\n",
        "    (alphas_best[:, np.newaxis] * alphas_best) * (y_train[:, np.newaxis] * y_train) * K_all\n",
        ")\n",
        "\n",
        "# Print the objective function value\n",
        "print(f\"Objective function value: {objective_value}\")\n",
        "\n",
        "\n",
        "# Define the prediction function\n",
        "def prediction(X, sv_X, sv_y, sv_alphas, b_best, gamma_best, p_best, kernel_func='gaussian'):\n",
        "    \"\"\"\n",
        "    Predict the labels for a dataset using the trained SVM model.\n",
        "\n",
        "    Parameters:\n",
        "    - X: np.array, the dataset (either train or test set) features\n",
        "    - sv_X: np.array, support vectors\n",
        "    - sv_y: np.array, labels of the support vectors\n",
        "    - sv_alphas: np.array, alphas of the support vectors\n",
        "    - b_best: float, bias term from training\n",
        "    - gamma_best: float, best gamma value for Gaussian kernel\n",
        "    - p_best: int, best degree for Polynomial kernel\n",
        "    - kernel_func: str, type of kernel ('gaussian' or 'polynomial')\n",
        "\n",
        "    Returns:\n",
        "    - predictions: np.array, predicted labels\n",
        "    \"\"\"\n",
        "\n",
        "    if kernel_func == 'gaussian':\n",
        "        K_test = gaussian_kernel_matrix(X, sv_X, gamma_best)\n",
        "    elif kernel_func == 'polynomial':\n",
        "        K_test = polynomial_kernel_matrix(X, sv_X, p_best)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported kernel function. Use 'gaussian' or 'polynomial'.\")\n",
        "\n",
        "    # Compute decision values\n",
        "    decision_values = np.dot(K_test, sv_alphas * sv_y) + b_best\n",
        "    predictions = np.sign(decision_values)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Make predictions on train and test set using the final model\n",
        "y_train_pred = prediction(X_train, sv_X, sv_y, sv_alphas, b_best, gamma_best, p_best, kernel_func=kernel_type)\n",
        "y_test_pred = prediction(X_test, sv_X, sv_y, sv_alphas, b_best, gamma_best, p_best, kernel_func=kernel_type)\n",
        "\n",
        "# Evaluate and print the accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print()\n",
        "print(f\"Training Set Accuracy: {train_accuracy:.2f}\")\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "\n",
        "# Confusion matrices\n",
        "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
        "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "print(\"Confusion Matrix (Training Set):\")\n",
        "print(conf_matrix_train)\n",
        "print()\n",
        "print(\"Confusion Matrix (Test Set):\")\n",
        "print(conf_matrix_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGcIUiNl3jz-",
        "outputId": "eb48a7eb-3efb-43a7-8969-0b36a1f501b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal bias term: -0.7868726923034077\n",
            "Best params: C = 1, gamma = 0.5, p = 2\n",
            "Objective function value: 126.15176987746685\n",
            "\n",
            "Training Set Accuracy: 0.92\n",
            "Test Set Accuracy: 0.92\n",
            "Confusion Matrix (Training Set):\n",
            "[[327  27]\n",
            " [ 30 316]]\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[134  12]\n",
            " [ 13 141]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SMO algorithm - Question 3**"
      ],
      "metadata": {
        "id": "lTwtiEibiEYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **gaussian kernel**"
      ],
      "metadata": {
        "id": "Td-1WeqDOaLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Gaussian kernel function\n",
        "def gaussian_kernel(X1, X2, gamma):\n",
        "    if X1.ndim == 1 and X2.ndim == 1:\n",
        "        return np.exp(-gamma * np.linalg.norm(X1 - X2) ** 2)\n",
        "    elif X1.ndim > 1 and X2.ndim == 1:\n",
        "        return np.exp(-gamma * np.linalg.norm(X1 - X2, axis=1) ** 2)\n",
        "    elif X1.ndim > 1 and X2.ndim > 1:\n",
        "        return np.exp(-gamma * np.linalg.norm(X1[:, np.newaxis] - X2[np.newaxis, :], axis=2) ** 2)\n"
      ],
      "metadata": {
        "id": "s2O9jc9vOnuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Objective function**"
      ],
      "metadata": {
        "id": "Z8YaqvKuOqPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute objective function\n",
        "def compute_objective_function(alpha, y, K):\n",
        "    return np.sum(alpha) - 0.5 * np.dot(alpha * y, np.dot(K, alpha * y))\n"
      ],
      "metadata": {
        "id": "V7y5qhYOOwVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SMO algorithm**"
      ],
      "metadata": {
        "id": "cGJVtMNsOxnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# SMO algorithm with MVP\n",
        "def smo_svm(X, y, C, gamma, tol=1e-5, max_passes=10):\n",
        "    n_samples = X.shape[0]\n",
        "    alpha = np.zeros(n_samples)\n",
        "    b = 0.0\n",
        "    passes = 0\n",
        "    total_iterations = 0\n",
        "\n",
        "    # Precompute the Kernel matrix\n",
        "    K = gaussian_kernel(X, X, gamma)\n",
        "\n",
        "    # Error cache\n",
        "    E = np.zeros(n_samples)\n",
        "\n",
        "    while passes < max_passes:\n",
        "        num_changed_alphas =0\n",
        "\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            # Calculate E[i]\n",
        "            total_iterations += 1\n",
        "            E[i] = np.sum((alpha * y) * K[:, i]) + b - y[i]\n",
        "\n",
        "            if (y[i]*E[i] < -tol and alpha[i] < C) or (y[i]*E[i] > tol and alpha[i] > 0):\n",
        "                # Select j (Most Violating Pair)\n",
        "                j = np.argmax(np.abs(E - E[i]))\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                # Save old alphas\n",
        "                alpha_i_old = alpha[i]\n",
        "                alpha_j_old = alpha[j]\n",
        "\n",
        "                # Compute L and H\n",
        "                if y[i] != y[j]:\n",
        "                    L = max(0, alpha[j] - alpha[i])\n",
        "                    H = min(C, C + alpha[j] - alpha[i])\n",
        "                else:\n",
        "                    L = max(0, alpha[i] + alpha[j] - C)\n",
        "                    H = min(C, alpha[i] + alpha[j])\n",
        "                if L == H:\n",
        "                    continue\n",
        "\n",
        "                # Compute eta\n",
        "                eta = 2.0 * K[i, j] - K[i, i] - K[j, j]\n",
        "                if eta >= 0:\n",
        "                    continue\n",
        "\n",
        "                # Update alpha[j]\n",
        "                alpha[j] -= y[j] * (E[i] - E[j]) / eta\n",
        "                # Clip alpha[j]\n",
        "                alpha[j] = np.clip(alpha[j], L, H)\n",
        "\n",
        "                # Check for significant change\n",
        "                if abs(alpha[j] - alpha_j_old) < tol:\n",
        "                    continue\n",
        "\n",
        "                # Update alpha[i]\n",
        "                alpha[i] += y[i]*y[j]*(alpha_j_old - alpha[j])\n",
        "\n",
        "                # Compute b1 and b2\n",
        "                b1 = b - E[i] - y[i]*(alpha[i] - alpha_i_old)*K[i, i] - y[j]*(alpha[j] - alpha_j_old)*K[i, j]\n",
        "                b2 = b - E[j] - y[i]*(alpha[i] - alpha_i_old)*K[i, j] - y[j]*(alpha[j] - alpha_j_old)*K[j, j]\n",
        "\n",
        "                # Update b\n",
        "                if 0 < alpha[i] < C:\n",
        "                    b = b1\n",
        "                elif 0 < alpha[j] < C:\n",
        "                    b = b2\n",
        "                else:\n",
        "                    b = (b1 + b2) / 2.0\n",
        "\n",
        "                num_changed_alphas += 1\n",
        "\n",
        "        if num_changed_alphas == 0:\n",
        "            passes += 1\n",
        "        else:\n",
        "            passes = 0\n",
        "\n",
        "    # Compute the final objective function value\n",
        "    obj = compute_objective_function(alpha, y, K)\n",
        "\n",
        "    # Count iterations (approximate)\n",
        "    num_iterations = passes\n",
        "\n",
        "    return alpha, b, obj, total_iterations\n",
        "\n",
        "# Set hyperparameters\n",
        "C = 10.0\n",
        "gamma = 0.5\n",
        "tol = 1e-5\n",
        "max_passes = 10\n",
        "\n",
        "# Compute initial objective function value\n",
        "n_samples_train = X_train.shape[0]\n",
        "K_train = gaussian_kernel(X_train, X_train, gamma)\n",
        "alpha_init = np.zeros(n_samples_train)\n",
        "obj_init = compute_objective_function(alpha_init, y_train, K_train)\n",
        "print(f\"Initial Objective Function Value: {obj_init:.4f}\")\n",
        "\n",
        "# Train the SVM\n",
        "alpha, b, obj_final, num_iterations = smo_svm(X_train, y_train, C, gamma, tol, max_passes)\n",
        "print(f\"Final Objective Function Value: {obj_final:.4f}\")\n",
        "print(f\"Number of Iterations (Passes): {num_iterations}\")\n",
        "print(f\"Number of Support Vectors: {np.sum(alpha > 0)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMS_65iaEhyI",
        "outputId": "b8eecca2-8b81-4528-f9cf-a3458ae6fa17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Objective Function Value: 0.0000\n",
            "Final Objective Function Value: 326.9523\n",
            "Number of Iterations (Passes): 15400\n",
            "Number of Support Vectors: 70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction**"
      ],
      "metadata": {
        "id": "MEsA2CdbPpDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction function\n",
        "def predict(X_train, y_train, X_test, alpha, b, gamma):\n",
        "    K = gaussian_kernel(X_test, X_train, gamma)\n",
        "    return np.sign(np.dot(K, alpha * y_train) + b)\n",
        "\n",
        "# Evaluate performance\n",
        "y_train_pred = predict(X_train, y_train, X_train, alpha, b, gamma)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "y_test_pred = predict(X_train, y_train, X_test, alpha, b, gamma)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiXIWGITPsj9",
        "outputId": "796121a5-2b04-4a2b-b241-f082c2eef4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 91.00%\n",
            "Test Accuracy: 90.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hyperparameter Tunning**"
      ],
      "metadata": {
        "id": "cyGVMJhZO6DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example hyperparameter tuning (simplified)\n",
        "start_time = time.time()\n",
        "\n",
        "best_accuracy = 0\n",
        "best_C = None\n",
        "best_gamma = None\n",
        "\n",
        "for gamma in [0.1, 0.5, 1.0]:\n",
        "    for C in [0.1, 1.0, 10.0]:\n",
        "        alpha, b, obj_final, num_iterations = smo_svm(X_train, y_train, C, gamma, tol, max_passes)\n",
        "        y_val_pred = predict(X_train, y_train, X_test, alpha, b, gamma)\n",
        "        val_accuracy = accuracy_score(y_test, y_val_pred)\n",
        "        print(f\"Gamma: {gamma}, C: {C}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            best_C = C\n",
        "            best_gamma = gamma\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution Time: {execution_time} seconds\")\n",
        "print()\n",
        "\n",
        "print(f\"Best Validation Accuracy: {best_accuracy * 100:.2f}% with C={best_C} and gamma={best_gamma}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twoPoNDGLmgE",
        "outputId": "d822aa12-d6c6-4a9f-de08-24e2a584a159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gamma: 0.1, C: 0.1, Validation Accuracy: 66.67%\n",
            "Gamma: 0.1, C: 1.0, Validation Accuracy: 80.67%\n",
            "Gamma: 0.1, C: 10.0, Validation Accuracy: 74.33%\n",
            "Gamma: 0.5, C: 0.1, Validation Accuracy: 67.33%\n",
            "Gamma: 0.5, C: 1.0, Validation Accuracy: 70.00%\n",
            "Gamma: 0.5, C: 10.0, Validation Accuracy: 90.33%\n",
            "Gamma: 1.0, C: 0.1, Validation Accuracy: 75.00%\n",
            "Gamma: 1.0, C: 1.0, Validation Accuracy: 69.33%\n",
            "Gamma: 1.0, C: 10.0, Validation Accuracy: 69.33%\n",
            "Execution Time: 6.03749942779541 seconds\n",
            "\n",
            "Best Validation Accuracy: 90.33% with C=10.0 and gamma=0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qBCVeW0nS4os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ethnicity Clasification**"
      ],
      "metadata": {
        "id": "E2VJBBH0S52f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eDyMW1zaU5MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**"
      ],
      "metadata": {
        "id": "udIIa2wNU5q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Replace 'your_file_id_here' with the actual file ID\n",
        "file_id = '1zffI34PfHh-27JdvUWN779ICYVaiXcuZ'\n",
        "# Read the CSV file into a DataFrame'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "\n",
        "df = pd.read_csv(download_url)"
      ],
      "metadata": {
        "id": "wS_MSUdHTDjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes_to_keep = [0 , 1 , 2]\n",
        "\n",
        "df = df[df['gt'].isin(classes_to_keep)]\n",
        "\n",
        "X = df.iloc[:, :-1].values  # last column is the label\n",
        "y = df.iloc[:, -1].values   # labels"
      ],
      "metadata": {
        "id": "012MjyZ9nfeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train , X_test , y_train , y_test = train_test_split(X , y, test_size = 0.3)"
      ],
      "metadata": {
        "id": "2wpNn5NGnw4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gaussian kernell**"
      ],
      "metadata": {
        "id": "BNBt2zIclN4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Gaussian kernel function\n",
        "def gaussian_kernel(X1, X2, gamma):\n",
        "    if X1.ndim == 1:\n",
        "        X1 = X1[np.newaxis, :]\n",
        "    if X2.ndim == 1:\n",
        "        X2 = X2[np.newaxis, :]\n",
        "    sq_dists = np.sum(X1**2, axis=1).reshape(-1, 1) + \\\n",
        "               np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)\n",
        "    return np.exp(-gamma * sq_dists)"
      ],
      "metadata": {
        "id": "mlqvrFF6lUlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training svm**"
      ],
      "metadata": {
        "id": "O2_5OendlWB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the SVM training function with Gaussian kernel\n",
        "def train_svm(X, y, C=1.0, gamma=0.05):\n",
        "\n",
        "    n_samples = X.shape[0]\n",
        "    K = gaussian_kernel(X, X, gamma)\n",
        "    P = matrix(np.outer(y, y) * K)\n",
        "    q = matrix(-np.ones(n_samples))\n",
        "    G_std = np.diag(-np.ones(n_samples))\n",
        "    h_std = np.zeros(n_samples)\n",
        "    G_slack = np.diag(np.ones(n_samples))\n",
        "    h_slack = C * np.ones(n_samples)\n",
        "    G = matrix(np.vstack((G_std, G_slack)))\n",
        "    h = matrix(np.hstack((h_std, h_slack)))\n",
        "    A = matrix(y.reshape(1, -1).astype('double'))\n",
        "    b = matrix(np.zeros(1))\n",
        "    solvers.options['show_progress'] = False\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    solution = solvers.qp(P, q, G, h, A, b)\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "\n",
        "    alphas = np.ravel(solution['x'])\n",
        "    sv = alphas > 1e-5\n",
        "    ind = np.arange(len(alphas))[sv]\n",
        "    alphas_sv = alphas[sv]\n",
        "    X_sv = X[sv]\n",
        "    y_sv = y[sv]\n",
        "    b_value = 0\n",
        "\n",
        "\n",
        "    for n in range(len(alphas_sv)):\n",
        "        b_value += y_sv[n]\n",
        "        b_value -= np.sum(alphas_sv * y_sv * K[ind[n], sv])\n",
        "    b_value /= len(alphas_sv)\n",
        "    iterations = solution['iterations']\n",
        "    objective = solution['primal objective']\n",
        "\n",
        "\n",
        "    model = {\n",
        "        'alphas': alphas_sv,\n",
        "        'X_sv': X_sv,\n",
        "        'y_sv': y_sv,\n",
        "        'b': b_value,\n",
        "        'C': C,\n",
        "        'gamma': gamma,\n",
        "        'training_time': training_time,\n",
        "        'iterations': iterations,\n",
        "        'objective': objective\n",
        "    }\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SZS9jN1Eb-RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction and Accuracy**"
      ],
      "metadata": {
        "id": "rWRDCV2glw7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the SVM classifiers with Gaussian kernel\n",
        "classes = np.unique(y_train)\n",
        "models = {}\n",
        "C = 1.0  # Regularization parameter\n",
        "gamma = 0.5  # Gaussian kernel hyperparameter\n",
        "\n",
        "for c in classes:\n",
        "    print(f\"Training classifier for class {c} vs. rest\")\n",
        "    y_binary = np.where(y_train == c, 1, -1)\n",
        "    model = train_svm(X_train, y_binary, C=C, gamma=gamma)\n",
        "    models[c] = model\n",
        "\n",
        "# Define prediction functions\n",
        "def project(model, X):\n",
        "    K = gaussian_kernel(X, model['X_sv'], model['gamma'])\n",
        "    y_predict = np.sum(model['alphas'] * model['y_sv'] * K, axis=1) + model['b']\n",
        "    return y_predict\n",
        "\n",
        "def predict(models, X):\n",
        "    predictions = np.zeros((X.shape[0], len(models)))\n",
        "    classes = list(models.keys())\n",
        "    for i, c in enumerate(classes):\n",
        "        model = models[c]\n",
        "        y_predict = project(model, X)\n",
        "        predictions[:, i] = y_predict\n",
        "    class_indices = np.argmax(predictions, axis=1)\n",
        "    y_pred = [classes[i] for i in class_indices]\n",
        "    return np.array(y_pred)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = predict(models, X_train)\n",
        "y_test_pred = predict(models, X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"\\nTraining accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "def confusion_matrix(y_true, y_pred, labels):\n",
        "    n_labels = len(labels)\n",
        "    cm = np.zeros((n_labels, n_labels), dtype=int)\n",
        "    label_to_index = {label: index for index, label in enumerate(labels)}\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        i = label_to_index[t]\n",
        "        j = label_to_index[p]\n",
        "        cm[i, j] += 1\n",
        "    return cm\n",
        "\n",
        "labels = classes\n",
        "cm = confusion_matrix(y_test, y_test_pred, labels)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Report metrics\n",
        "print(f\"\\nHyperparameters:\")\n",
        "print(f\"C (Regularization parameter): {C}\")\n",
        "print(f\"Gamma (Gaussian kernel parameter): {gamma}\")\n",
        "print(\"Kernel: Gaussian (RBF)\")\n",
        "\n",
        "total_training_time = sum(model['training_time'] for model in models.values())\n",
        "print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
        "\n",
        "total_iterations = sum(model['iterations'] for model in models.values())\n",
        "print(f\"Total number of optimization iterations: {total_iterations}\")\n",
        "\n",
        "for c, model in models.items():\n",
        "    print(f\"Class {c} SVM dual objective value: {model['objective']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBA0nlCHieMu",
        "outputId": "86472d6d-4f77-448a-e89f-c11541ae10c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier for class 0 vs. rest\n",
            "Training classifier for class 1 vs. rest\n",
            "Training classifier for class 2 vs. rest\n",
            "\n",
            "Training accuracy: 94.76%\n",
            "Test accuracy: 89.56%\n",
            "\n",
            "Confusion Matrix:\n",
            "[[137   9  10]\n",
            " [  8 140   7]\n",
            " [  8   5 126]]\n",
            "\n",
            "Hyperparameters:\n",
            "C (Regularization parameter): 1.0\n",
            "Gamma (Gaussian kernel parameter): 0.5\n",
            "Kernel: Gaussian (RBF)\n",
            "Total training time: 8.05 seconds\n",
            "Total number of optimization iterations: 33\n",
            "Class 0 SVM dual objective value: -167.67525784070426\n",
            "Class 1 SVM dual objective value: -136.66189889939014\n",
            "Class 2 SVM dual objective value: -152.0289143906552\n"
          ]
        }
      ]
    }
  ]
}